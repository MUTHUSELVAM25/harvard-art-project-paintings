{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61284ae9",
   "metadata": {},
   "source": [
    "HARVARD'S PROJECT - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65342015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a812dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '49dcb8ef-cc75-4e72-b607-4d4873b0572f'\n",
    "url = \"https://api.harvardartmuseums.org/classification\"\n",
    "params = {\n",
    "    \"apikey\": API_KEY,\n",
    "    \"classification\": \"Paintings\",  \n",
    "    \"size\": 100,  \n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "response = requests.get(url, params)\n",
    "data = response.json()\n",
    "\n",
    "print(data['records'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "record=data['records']\n",
    "for record in data['records']:\n",
    "    if record['objectcount']>=2500:\n",
    "        print(record['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed51d1",
   "metadata": {},
   "source": [
    "API INTEGRATION & BASIC DATA FETCHING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API integration & basic data fetching for one classificatio -> \"Painting\"\n",
    "\n",
    "API_KEY = \"49dcb8ef-cc75-4e72-b607-4d4873b0572f\"                        #This stores the API key\n",
    "BASE_URL = \"https://api.harvardartmuseums.org/object\"                   #This base URL is the address of Harvard‚Äôs API for object data, where we send our requests.\n",
    "\n",
    "classification = \"Paintings\"                                            #this is  choosing a classification type (\"Paintings\") of artifacts to fetch #You can change this to another classification if needed #You can later allow this to be selected from a dropdown menu in Streamlit.\n",
    "all_records = []                                                        #all_records: an empty list to store all the artifact results.\n",
    "records_needed = 2500                                                   #records_needed = 2500: how many artworks you want to collect.\n",
    "page = 1                                                                #page = 1: we‚Äôll start from the first page of results.\n",
    "\n",
    "while len(all_records) < records_needed:                                #This loop will continue fetching pages of data until you get all 2500+ records.\n",
    "    params = {                                                          #These are parameters that you send to the API.\n",
    "        \"apikey\": API_KEY,                                              #apikey: your unique key to access the API.\n",
    "        \"classification\": classification,                               #classification: filter results (e.g., Paintings only).\n",
    "        \"size\": 100,                                                    #size: get 100 records at a time.   # Maximum allowed per call\n",
    "        \"page\": page,                                                   #page: which page to load.\n",
    "        \"hasimage\": 1                                                   #hasimage = 1: only include items that have an image attached.        # Only objects with images\n",
    "    }\n",
    "    print(f\"Fetching page {page}...\")                                   #This tells you in the terminal which page the script is currently fetching. Just for tracking.\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)                    #Sends the request to Harvard‚Äôs server and gets the data.\n",
    "    data = response.json()                                              #.json() converts the server‚Äôs response into a usable dictionary in Python.\n",
    "\n",
    "     # Add the current page's records to our collection                 \n",
    "    all_records.extend(data[\"records\"])                                 #The API returns one page of 100 records in data[\"records\"]  #extend() adds these to your full all_records list.\n",
    "\n",
    "    # If there's no next page, stop the loop\n",
    "    if data[\"info\"][\"next\"] is None:                                    #Checks if this is the last page of data.\n",
    "        break                                                           #If there's no next page, the loop stops.\n",
    "\n",
    "    page += 1                                                           #Goes to the next page (page = 2, then 3, etc.)\n",
    "    time.sleep(1)                                                       #Waits 1 second before next request to avoid spamming the API.\n",
    "\n",
    "print(f\"\\n‚úÖ Collected {len(all_records)} records.\")                   #Shows the total number of records collected after finishing the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cdd3bf",
   "metadata": {},
   "source": [
    "PYTHON CODE FOR EXTRACTING AND ORGANIZING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88531836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METADATA TABLE: PRD-compliant version\n",
    "\n",
    "metadata_rows = []                                     #Creates an empty list called metadata_rows that will store the information (rows) for each artifact's metadata.\n",
    "\n",
    "for record in all_records:                             #Loops through every artifact record you've collected in all_records.    \n",
    "    metadata_rows.append({                             #Makes a new dictionary for each artifact, with keys like id, title, etc.  #Adds (appends) this dictionary to the metadata_rows list.\n",
    "        'id': record.get('id'),                        # primary key\n",
    "        'title': record.get('title'),\n",
    "        'culture': record.get('culture'),\n",
    "        'period': record.get('period'),\n",
    "        'century': record.get('century'),\n",
    "        'medium': record.get('medium'),               #Uses .get('...') to safely extract each field from the record (returns None if the field is missing).\n",
    "        'dimensions': record.get('dimensions'),\n",
    "        'department': record.get('department'),\n",
    "        'description': record.get('description'),\n",
    "        'classification': record.get('classification'),\n",
    "        'accessionyear': record.get('accessionyear'),\n",
    "        'accessionmethod': record.get('accessionmethod'),\n",
    "    })\n",
    "\n",
    "# DataFrame and preview\n",
    "df_metadata = pd.DataFrame(metadata_rows)               #Creates a pandas DataFrame (like an Excel table) called df_metadata using the list of dictionaries you just built.\n",
    "print(df_metadata.head())                               #Shows the first 5 rows of your metadata table in the output‚Äîso you can quickly check your data.\n",
    "\n",
    "# (Save to CSV for inseration use)\n",
    "df_metadata.to_csv('artifact_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # MEDIA TABLE: PRD-compliant version (final)\n",
    " \n",
    "# all_records = your full list of artifact records (dicts) from the API, same as metadata_table source\n",
    "\n",
    "media_rows = []                                                                                 #Creates an empty list called media_rows that will store the information (rows) for each artifact's metadata.\n",
    "\n",
    "for record in all_records:                                                                      #Loops through every artifact record you've collected in all_records     \n",
    "    # Always use this record's ID from metadata as the media-table objectid\n",
    "    objectid = record.get('id')\n",
    "    \n",
    "    # Handle images/media/colors fields; if missing, default to empty list\n",
    "    images = record.get('images', []) if record.get('images') is not None else []\n",
    "    media = record.get('media', []) if record.get('media') is not None else []\n",
    "    colors = record.get('colors', []) if record.get('colors') is not None else []\n",
    "    \n",
    "    # Count them (default to 0 if list is empty)\n",
    "    imagecount = len(images)\n",
    "    mediacount = len(media)\n",
    "    colorcount = len(colors)\n",
    "    \n",
    "    media_rows.append({\n",
    "        'objectid': objectid,                  # FK to artifact_metadata.id\n",
    "        'imagecount': imagecount,\n",
    "        'mediacount': mediacount,\n",
    "        'colorcount': colorcount,\n",
    "        'rank': record.get('rank'),\n",
    "        'datebegin': record.get('datebegin'),                                                  #Uses .get('...') to safely extract each field from the record (returns None if the field is missing).                   \n",
    "        'dateend': record.get('dateend'),\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_media = pd.DataFrame(media_rows)                                                            #Creates a pandas DataFrame (like an Excel table) called df_media using the list of dictionaries you just built\n",
    "print(df_media.head())                                                                         #Shows the first 5 rows of your metadata table in the output‚Äîso you can quickly check your data.\n",
    "\n",
    "# (Save to CSV for insertion use)\n",
    "df_media.to_csv('artifact_media.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLORS TABLE: PRD-compliant version\n",
    "\n",
    "# Build a set of valid metadata IDs to enforce FK integrity\n",
    "metadata_ids = set(df_metadata['id'])   # If df_metadata exists, else rebuild from all_records\n",
    "\n",
    "color_rows = []\n",
    "for record in all_records:                                               #Loops through every artifact record you've collected in all_records \n",
    "    if 'colors' in record and record.get('id') in metadata_ids:\n",
    "        for c in record['colors']:\n",
    "            color_rows.append({\n",
    "                'objectid': record.get('id'),          # foreign key to artifact_metadata.id\n",
    "                'color': c.get('color'),\n",
    "                'spectrum': c.get('spectrum'),\n",
    "                'hue': c.get('hue'),\n",
    "                'percent': c.get('percent'),           # float\n",
    "                'css3': c.get('css3'),                 # CSS3 hex or name\n",
    "            })\n",
    "\n",
    "df_colors = pd.DataFrame(color_rows)                                      #Creates a pandas DataFrame (like an Excel table) called df_media using the list of dictionaries you just built\n",
    "print(df_colors.head())                                                   #Shows the first 5 rows of your metadata table in the output‚Äîso you can quickly check your data.\n",
    "\n",
    "# (Save to CSV for inseration use)\n",
    "df_colors.to_csv('artifact_colors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK AND PREVIEW THE DATA:\n",
    "\n",
    "print(df_metadata.head())               #Shows the first 5 rows of your metadata table in the output‚Äîso you can quickly check your data.\n",
    "print(df_media.head())                  #Displays the first five rows of the df_media DataFrame, just to check that your data looks correct.\n",
    "print(df_colors.head())                 #Displays the first five rows of your colors DataFrame so you can check your extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905bbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE EACH TABLE AS CSV:\n",
    "\n",
    "df_metadata.to_csv('artifact_metadata.csv', index=False)     \n",
    "df_media.to_csv('artifact_media.csv', index=False)           \n",
    "df_colors.to_csv('artifact_colors.csv', index=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713eb47",
   "metadata": {},
   "source": [
    "UPLOADING THE DATAFRAMES(CSV FILES) INTO TiDB CLOUD MYSQL TABLES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install MySQL pyton library:\n",
    "\n",
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d760d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to visual studio python -> TiDB cloud\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "  host = \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\",\n",
    "  port = 4000,\n",
    "  user = \"EnD345nfx9wxmnG.root\",\n",
    "  password = \"Q4KKSkNgKxF3JIPn\",\n",
    ")\n",
    "mycursor = connection.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83209fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"create database HARVARD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "684451bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"use HARVARD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create artifacts_metadata table:\n",
    "\n",
    "mycursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artifact_metadata (\n",
    "    id INT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    culture TEXT,\n",
    "    period TEXT,\n",
    "    century TEXT,\n",
    "    medium TEXT,\n",
    "    dimensions TEXT,\n",
    "    description TEXT,\n",
    "    department TEXT,\n",
    "    classification TEXT,\n",
    "    accessionyear INT,\n",
    "    accessionmethod TEXT\n",
    ")\n",
    "\"\"\")\n",
    "connection.commit()\n",
    "print(\"artifact_metadata table created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a46f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create artifacts_media table:\n",
    "\n",
    "mycursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artifact_media (\n",
    "    objectid INT,\n",
    "    imagecount INT,\n",
    "    mediacount INT,\n",
    "    colorcount INT,\n",
    "    `rank` INT,\n",
    "    datebegin INT,\n",
    "    dateend INT\n",
    ")\n",
    "\"\"\")\n",
    "connection.commit()\n",
    "print(\"artifact_media table created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create artifacts_colors table:\n",
    "\n",
    "mycursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artifact_colors (\n",
    "    objectid INT,\n",
    "    color TEXT,\n",
    "    spectrum TEXT,\n",
    "    hue TEXT,\n",
    "    percent FLOAT,\n",
    "    css3 TEXT\n",
    ")\n",
    "\"\"\")\n",
    "connection.commit()\n",
    "print(\"artifact_colors table created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a481c",
   "metadata": {},
   "source": [
    "THE FETCHED DATA WILL BE INSERTED TO THE CREATED SQL TABLES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3102da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert complete (duplicates skipped).\n"
     ]
    }
   ],
   "source": [
    "#The fetched artifacts_metadata was insereted in created sql table:\n",
    "\n",
    "# --- Read & Clean Data ---\n",
    "df = pd.read_csv('artifact_metadata.csv')\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.drop_duplicates(subset=['id'])   # <--- Only unique IDs\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "    INSERT INTO artifact_metadata\n",
    "    (id, title, culture, period, century, medium, dimensions, department, description, classification, accessionyear, accessionmethod)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        mycursor.execute(sql, tuple(row))\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print(f\"Duplicate id: {row['id']}, skipping.\")\n",
    "        else:\n",
    "            print(f\"Error inserting row: {row['id']} -> {err}\")\n",
    "\n",
    "connection.commit()\n",
    "print(\"Insert complete (duplicates skipped).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The fetched artifacts_media was insereted in created sql table:\n",
    "\n",
    "# Step 1: Load your CSV into a DataFrame\n",
    "df = pd.read_csv('artifact_media.csv')\n",
    "\n",
    "# Step 2: Convert all columns to object type (for safe null handling)\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)  # Fix NaN problem\n",
    "\n",
    "# Step 3: Remove duplicate 'objectid' rows (keep first one only)\n",
    "df = df.drop_duplicates(subset=['objectid'])  # Adjust key if needed\n",
    "\n",
    "# Step 4: Define your SQL insert statement\n",
    "sql = \"\"\"\n",
    "INSERT INTO artifact_media\n",
    "(objectid, imagecount, mediacount, colorcount, `rank`, datebegin, dateend)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Step 5: Insert each row, skip duplicates, print errors\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        mycursor.execute(sql, tuple(row))\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:  # Duplicate PK (objectid) error code\n",
    "            print(f\"Duplicate objectid: {row['objectid']}, skipping.\")\n",
    "        else:\n",
    "            print(f\"Error inserting row: {row['objectid']} -> {err}\")\n",
    "\n",
    "connection.commit()\n",
    "print(\"Insert finished (duplicates skipped, no NaN error)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170463e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The fetched artifacts_colors was insereted in created sql table:\n",
    "\n",
    "# Step 1: Load the CSV into a DataFrame\n",
    "df = pd.read_csv('artifact_colors.csv')\n",
    "\n",
    "# Step 2: Ensure all columns allow None, replace NaN with None (handles missing values)\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "# Step 3: Remove exact duplicate rows (optional but common with colors/colors)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Step 4: Prepare your INSERT SQL statement\n",
    "sql = \"\"\"\n",
    "INSERT INTO artifact_colors\n",
    "(objectid, color, spectrum, hue, percent, css3)\n",
    "VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Step 5: Prepare data as a list of tuples for batch insertion\n",
    "data = [tuple(row) for _, row in df.iterrows()]\n",
    "\n",
    "# Step 6: Batch insert for FAST loading\n",
    "try:\n",
    "    mycursor.executemany(sql, data)\n",
    "    connection.commit()\n",
    "    print(f\"Inserted {mycursor.rowcount} rows successfully into artifact_colors!\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error during batch insert: {err}\")\n",
    "\n",
    "# Step 7: Clean up\n",
    "mycursor.close()\n",
    "connection.close()\n",
    "print(\"All done! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT COUNT(*) FROM artifact_metadata\"\n",
    "mycursor.execute(query)\n",
    "result = mycursor.fetchone()\n",
    "print(\"Row count in artifact_metadata:\", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT COUNT(*) FROM artifact_media\"\n",
    "mycursor.execute(query)\n",
    "result = mycursor.fetchone()\n",
    "print(\"Row count in artifact_media:\", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT COUNT(*) FROM artifact_colors\"\n",
    "mycursor.execute(query)\n",
    "result = mycursor.fetchone()\n",
    "print(\"Row count in artifact_colors:\", result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44169c",
   "metadata": {},
   "source": [
    "STREAMLIT UI DESIGN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f09e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# ---------------------------------------- Database Connection --------------------------------------------------------\n",
    "def get_connection():\n",
    "    return mysql.connector.connect(\n",
    "        host='gateway01.ap-southeast-1.prod.aws.tidbcloud.com',\n",
    "        port=4000,\n",
    "        user='EnD345nfx9wxmnG.root',\n",
    "        password='Q4KKSkNgKxF3JIPn',\n",
    "        database='HARVARD'\n",
    "    )\n",
    "\n",
    "# ------------------------------------------ Start Streamlit UI --------------------------------------------------------\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.markdown(\"<h3 style='color: black;'>üñºÔ∏è Harvard Artifacts Collections</h3>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------ App Navigation Menu -------------------------------------------------------\n",
    "\n",
    "menu = st.radio(\"Choose a mode:\", [\"Classification Search\", \"SQL Queries\"], horizontal=True)\n",
    "\n",
    "conn = get_connection()\n",
    "cur = conn.cursor()\n",
    "\n",
    "if menu == \"Classification Search\":\n",
    "    st.subheader(\"üîé Search by Classification (Ex: Coins, Paintings, ...)\")\n",
    "    \n",
    "    #--------------------------------------- Fetch classification options from database ---------------------------------------------------\n",
    "    \n",
    "    cur.execute(\"SELECT DISTINCT classification FROM artifact_metadata WHERE classification IS NOT NULL AND classification <> ''\")\n",
    "    classes = sorted([row[0] for row in cur.fetchall()])\n",
    "    selected_class = st.selectbox(\"Select classification\", [\"(Show all)\"] + classes)\n",
    "\n",
    "\n",
    "\n",
    "    if st.button(\"Show Artifacts\"):\n",
    "        if selected_class == \"(Show all)\":\n",
    "            cur.execute(\"SELECT * FROM artifact_metadata LIMIT 100\")\n",
    "            st.info(\"Showing up to 100 results. Filter by classification for more precise results.\")\n",
    "        else:\n",
    "            cur.execute(\"SELECT * FROM artifact_metadata WHERE classification = %s\", (selected_class,))\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        if df.empty:\n",
    "            st.warning(\"No data found for this classification.\")\n",
    "        else:\n",
    "            st.dataframe(df)\n",
    "\n",
    "\n",
    "\n",
    "elif menu == \"SQL Queries\":\n",
    "    st.subheader(\"üìÑ Run PRD/Custom SQL Queries\")\n",
    "    query_dict = {\n",
    "        \"List all artifacts from 14th century belonging to Japanese culture\":\n",
    "            \"SELECT * FROM artifact_metadata WHERE century = '14th century' AND culture = 'Japnaese'\",\n",
    "        \"What are the unique cultures represented in the artifacts?\":\n",
    "            \"SELECT DISTINCT culture FROM artifact_metadata WHERE culture IS NOT NULL AND culture <> ''\",\n",
    "        \"List all artifacts from the British Period\":\n",
    "            \"SELECT * FROM artifact_metadata WHERE period = 'Brititsh Period'\",\n",
    "        \"List artifact titles ordered by accession year (descending)\":\n",
    "            \"SELECT title, accessionyear FROM artifact_metadata ORDER BY accessionyear DESC\",\n",
    "        \"How many artifacts are there per department?\":\n",
    "            \"SELECT department, COUNT(*) AS artifact_count FROM artifact_metadata GROUP BY department ORDER BY artifact_count DESC\",\n",
    "        \"Which artifacts have more than 1 image?\":\n",
    "            \"SELECT m.id, m.title,a.imagecount FROM artifact_metadata m JOIN artifact_media a ON m.id = a.objectid WHERE a.imagecount > 1\",\n",
    "        \"What is the average rank of all artifacts?\":  \n",
    "            \"SELECT AVG(rank_num) AS average_rank FROM artifact_media\",\n",
    "        \"Which artifacts have a higher mediacount than colorcount?\":    \n",
    "            \"SELECT m.id, m.title, a.mediacount, a.colorcount FROM artifact_metadata m JOIN artifact_media a ON  m.id = a.objectid WHERE a.colorcount > a.mediacount\",\n",
    "        \" List all artifacts created between 1500 and 1600\":\n",
    "            \"SELECT * FROM artifact_media WHERE datebegin >= 1500 AND dateend <= 1600\",\n",
    "        \"How many artifacts have no media files?\":\n",
    "            \"SELECT COUNT(*) AS no_media_count FROM artifact_media WHERE mediacount = 0\",\n",
    "        \"What are all the distinct hues used in the dataset?\":\n",
    "            \"SELECT DISTINCT hue FROM artifact_colors\", \n",
    "        \"What are the top 5 most used colors by frequency?\":\n",
    "            \"SELECT color, COUNT(*) AS frequency FROM artifact_colors GROUP BY color ORDER BY frequency DESC limit 5\",\n",
    "        \"What is the average coverage percentage for each hue\":\n",
    "            \"SELECT hue,AVG(percent) AS avg_coverage FROM artifact_colors GROUP BY hue ORDER BY avg_coverage DESC\",\n",
    "        \"List all colors used for a given artifact ID\":\n",
    "            \"SELECT color FROM artifact_colors WHERE objectid = '1429'\",\n",
    "        \"What is the total number of color entries in the dataset?\":\n",
    "            \"SELECT COUNT(*) AS total_color_entries FROM artifact_colors\",\n",
    "        \"List artifact titles and hues for all artifacts belonging to the Byzantine culture\":\n",
    "            \"SELECT m.title, c.hue FROM artifact_metadata m JOIN artifact_colors c ON m.id = c.objectid WHERE m.culture = 'Byzantine'\",\n",
    "        \"List each artifact title with its associated hues\":\n",
    "            \"SELECT m.title, GROUP_CONCAT(DISTINCT c.hue) AS hues FROM artifact_metadata m JOIN artifact_colors c ON m.id = c.objectid GROUP BY m.title\",\n",
    "        \"Get artifact titles, cultures, and media ranks where the period is not null\":\n",
    "            \"SELECT m.title, m.culture, a.rank FROM artifact_metadata m JOIN artifact_media a ON m.id = objectid WHERE m.period IS NOT NULL\",\n",
    "        \"Find artifact titles ranked in the top 10 that include the color hue Grey\":    \n",
    "            \"SELECT DISTINCT (m.title),a.rank_num FROM artifact_metadata m JOIN artifact_media a ON m.id = a.objectid JOIN artifact_colors c ON m.id = c.objectid WHERE c.hue = 'Grey' ORDER BY a.rank_num LIMIT 10\",       \n",
    "        \"How many artifacts exist per classification, and what is the average media count for each?\":\n",
    "            \"SELECT m.classification, COUNT(*) AS artifact_count, AVG(a.mediacount) AS avg_media_count FROM artifact_metadata m JOIN artifact_media a ON m.id = a.objectid GROUP BY m.classification ORDER BY artifact_count DESC\",\n",
    "        \"Find all artifacts first accessioned before the year 1900?\":\n",
    "            \"SELECT id, title, accessionyear FROM artifact_metadata WHERE accessionyear < 1900 ORDER BY accessionyear\",\n",
    "        \"Show all artifact titles created in the 20th century?\":\n",
    "            \"SELECT id, title, medium FROM artifact_metadata WHERE medium LIKE '%Gold%'\",\n",
    "        \"Count how many artifacts were made by the culture Chinese ?\":\n",
    "            \"SELECT COUNT(*) FROM artifact_metadata WHERE culture = 'Chinese'\",\n",
    "        \"List all artifacts whose title contains the word portrait (case-insensitive) ?\":\n",
    "            \"SELECT id, title FROM artifact_metadata WHERE LOWER(title) LIKE '%portrait%'\"                                              \n",
    "    }\n",
    "    \n",
    "    option = st.selectbox(\n",
    "        \"Choose a prewritten SQL query:\",\n",
    "        list(query_dict.keys())\n",
    "    )\n",
    "    default_sql = query_dict[option]\n",
    "    user_sql = st.text_area(\"SQL to run:\", value=default_sql, height=100)\n",
    "\n",
    "\n",
    "    if st.button(\"Run Query\"):\n",
    "        try:\n",
    "            cur.execute(user_sql)\n",
    "            rows = cur.fetchall()\n",
    "            columns = [desc[0] for desc in cur.description]\n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "            if df.empty:\n",
    "                st.info(\"No data found for this query.\")\n",
    "            else:\n",
    "                st.dataframe(df)\n",
    "        except Exception as e:\n",
    "            st.error(f\"SQL Error: {e}\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
